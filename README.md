# TAITANIC
1. 결측치 처리
> 🧞‍♀️ 시각화 등으로 결측치를 확인해주세요. 각 열의 결측치를 대체한 방법과 왜 그 방법을 사용했는지에 관하여 논리적으로 서술해주세요.
- - - 


2. 데이터 인코딩 범주형 변수는 모델이 이해할 수 있는 숫자 형태로 변환해야 합니다. 대표적인 방법으로는 Label Encoding(위계가 있는 경우)과 One-Hot Encoding(순서가 없는 경우)이 있습니다. 타겟 인코딩은 범주형 피처를 타겟 변수의 평균이나 비율로 인코딩하는 방법입니다. 타겟 변수가 특정 값일 확률을 사용하기도 합니다.
> 🧞‍♀️ 범주형 변수에 대해서 인코딩을 수행하셨다면 어떤 칼럼에 대해, 어떤 인코딩 방법을 사용하였는지 논리적으로 설명해주세요. (이때, 고려했던 인코딩 방법과 선택하지 않은 이유를 함께 제시해주세요.)
- - - 
one-hot encoding: 일종의 변수를 1과 0으로 나누는 방법으로 1은 그렇다, 0은 그렇지 않다. 국가 같이 순서가 없고 고유값이 많지 않으면 효율적. 변수의 의미를 정확하게 파악한다는 장점 보유. 그러나 COLuMN이 늘어난다는 단점 보유.

Label encoding:  순서가 있고, 고유값이 많을 때 효율적. 그러나, 중복된 값에 대한 인코딩이 복잡하다. 

frequency encoding: 해당 값이 "몇 번 등장했는지"

Target encoding: 해당 값의 "Target의 Ratio가 얼마나 되는지"

빈도 및 타겟 인코딩은 값이 겹친다는 단점이 있지만, 어떤 카테고리가 인기가 많은지, 평균이 얼마인지 확인할 수 있는 지표가 됨.

* 참고 <https://eda-ai-lab.tistory.com/569>


3. 데이터 스케일링
> 🧞‍♀️ 해당 데이터의 정규화 필요성에 대하여 논의해주세요. 해당 분포를 모델링에 적합한 형태로 변환하기 위해 사용한 스케일링 방식에 대해 설명해주세요. (ex. MinMax 스케일링은 이상치에 관하여 민감하므로, 그렇지 않은 ~ 방법을 사용했습니다.)
- - - 
데이터 스케일링(Data Scaling): 서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업을 의미합니다. 값을 조정하는 과정이기 때문에 수치형 변수에만 적용해야 합니다. 특성별로 데이터의 스케일이 다름.
보통 정규화 OR 표준화를 이용함.
정규화 : 특성들을 특정 범위(주로 [0,1]) 로 스케일링 

표준화: 평균 0, 분산 1로 정규분포 형성

scaler 는 fit 과 transform 메서드를 지니고 있음.
fit 메서드: 훈련 데이터에만 적용해, 훈련 데이터의 분포를 먼저 학습하고, 그 이후, transform 메서드를 훈련 데이터와 테스트 데이터에 적용해 스케일을 조정해야 함. 
따라서, 훈련 데이터에는 fit_transform() 메서드를 적용하고, 테스트 데이터에는 transform() 메서드를 적용해야합니다.
fit_transform() 은 fit 과 transform 이 결합된 단축 메서드. 

* 참고: <https://wooono.tistory.com/96> 


4. 데이터 왜도 왜도(skewness) 처리: 데이터가 비대칭적으로 분포되어 있을 때 로그 변환, 박스-콕스(Box-Cox) 변환 등을 사용해 왜도를 줄일 수 있습니다. 왜도가 크면 모델의 성능이 저하될 수 있습니다.
> 🧞‍♀️ 어떤 경우에 어떤 왜도 처리를 하는지 공부해보세요. 해당 데이터의 분포를 나타내는 시각화를 진행하고, 왜도를 수치로 표현해보세요. 타이타닉 데이터셋의 각 수치형 컬럼에는 왜도 처리가 필요한가요? 왜 그렇게 판단했나요?
- - - 
로그 변환: Numpy를 이용해 원하는 컬럼에 log() 함수를 호출하는 것

루트 제곱근 변환: Numpy의 sqrt() 함수를 호출하여 Numpy를 통해 루트 변환 (데이터의 최소값이 0보다 커야 한다)

박스 콕스 변환: scipy 라이브러리를 통해서 사용 (데이터값이 양수여야 한다)

* 참고: <https://dining-developer.tistory.com/18>


5. 이상치(Outliers) 이상치 처리: 이상치는 모델에 부정적인 영향을 줄 수 있기 때문에 이를 식별하고 처리하는 것이 중요합니다. 이상치를 제거하거나, 다른 값으로 대체(예: 중앙값)하는 방법이 있습니다. 이상치가 반드시 잘못된 것은 아니므로 주의해야 합니다.
> 🧞‍♀️ 이상치를 처리하였나요? 어떤 기준으로 이상치를 판단하였나요?
- - - 


6. 피처 선택 및 생성 피처 선택(Feature Selection): 모델 성능을 개선하거나 과적합을 방지하기 위해 불필요한 피처를 제거합니다. 이를 위해 통계적 방법(예: p-value), 모델 기반 방법(예: L1 정규화), 또는 피처 중요도를 활용합니다.
피처 생성(Feature Engineering): 모델 성능을 높이기 위해 새로운 피처를 생성할 수 있습니다. 예를 들어, 두 피처를 곱하거나 나누어 새로운 피처를 만들 수 있습니다.
> 🧞‍♀️ 새로 만든 피쳐가 있다면 자랑해주세요.
> 🧞‍♀️ 다중공선성이 있다고 판단되는 지표가 있었나요? 왜 그렇게 생각하셨으며, 어떻게 처리하셨나요?
- - - 
피쳐 추출: 피쳐들 사이에 내재한 특성이나 관계를 분석하여 이들을 잘 표현할 수 있는 새로운 선형 혹은 비선형 결합 변수를 만들어 데이터를 줄이는 방법. PCA(주성분 분석), LDA(선형 판별 분석) 등

피쳐 선택: 피쳐 중 타겟에 가장 관련성이 높은 피쳐만을 선정하여 피쳐의 수를 줄이는 방법. Filter, Wrapper, Embedded 메서드

종속변수의 활용여부에 따라
Supervised: 종속변수를 활용하여 선택한다.
Unsupervised: 종속변수를 활용하지 않고 선택한다.

선택 방법론에 따라서
Filter: 통계적 방법을 활용하여 선택
Wrapper: 모델을 활용하여 선택
Embedded: 모델 훈련과정에서 자동적으로 선택
Hybrid: Filter와 Wrapper 방식을 혼합


다중공선성: 두 설명 변수가 완전한 독립 변수가 아니기에, 회귀 분석을 완벽하게 설명할 수 없는 경우.
두 변수 중 하나는 유의한 변수로 드러나겠지만, 다른 하나는 불안정한 계수를 보일 것
p-value가 유의 수준보다 작아야 귀무가설이 귀각되고, 검정통계량이 클수록 P-value가 작아짐 (추정되는 회귀 계수 - 0) / 표준 오차 = 검정 통계량

* 참고 <https://velog.io/@happycherry6/%ED%94%BC%EC%B3%90%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81>
* 참고 <https://blog.naver.com/vnf3751/220833952857>


7. 데이터 분할 훈련/검증/테스트 데이터 분할: 데이터셋을 훈련, 검증, 테스트 세트로 나누어 모델의 성능을 평가합니다. 일반적으로 70-80%를 훈련 데이터로 사용하고, 나머지를 검증과 테스트 데이터로 사용합니다.
> 🧞‍♀️ 데이터 분할 방법 중 K-Fold Cross-Validation과 Stratified K-fold Cross Validation에 대해 공부해보세요. 최종적으로 어떤 방법을 사용했는지, 왜 사용했는지에 대해 서술해주세요.
- - -
Holdout: 빠른 속도로 검증 가능

K-Fold: K번의 Holdout으로 데이터를 분리하는 방법, 데이터셋 내의 모든 데이터를 훈련에 활용 가능

Stratified K-Fold: Y의 분포가 동일하기에, 불균형에서 오는 부분이 일부 해소

* 참고 <https://velog.io/@pluto_0905/ML-%EB%8B%A4%EC%96%91%ED%95%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%ED%95%A0-%EB%B0%A9%EB%B2%95>


8. 모델 선택 모델 유형 선택: 데이터의 특성에 따라 적합한 모델을 선택해야 합니다. 예를 들어, 선형 데이터에는 선형 회귀, 비선형 데이터에는 트리 기반 모델 등이 효과적일 수 있습니다.
> 🧞‍♀️ 해당 데이터가 예측하고자 하는 값이 어떤 유형인지에 관련하여 모델 선정의 논리성을 증명해주세요. 또한, 차안으로 선택할 수 있는 모델이 있다면 추천해주세요.
하이퍼파라미터 튜닝: 각 모델의 하이퍼파라미터를 최적화하여 성능을 극대화할 수 있습니다. 그리드 서치(Grid Search)나 랜덤 서치(Random Search) 등의 방법이 도모됩니다.
- - -
하이퍼파라미터의 예시
학습률(Learning Rate): 학습 과정에서 모델이 얼마나 빠르게 학습할지를 결정합니다.
트리의 깊이(Tree Depth) 및 노드 수(Node Count): 의사결정트리 또는 랜덤 포레스트와 같은 트리 기반 모델에서 중요합니다.
배치 크기(Batch Size) 및 에포크 수(Epochs): 신경망에서 사용되며, 학습 데이터의 처리 방법을 결정합니다.
규제 매개변수(Regularization Parameters): 과적합을 방지하기 위해 모델의 복잡성을 조절합니다.
* 참고 <https://dream-of-dev.tistory.com/47>


9. 모델 평가 평가 지표 선택: 회귀 문제에서는 MSE, MAE, R², 분류 문제에서는 정확도, 정밀도, 재현율, F1-score 등을 사용하여 모델 성능을 평가합니다. 교차 검증(Cross-Validation): 데이터를 여러 번 분할하여 모델을 평가하는 방법입니다. 이를 통해 모델의 일반화 성능을 더 잘 평가할 수 있습니다.
평가 지표의 종류에 대해 공부해보세요.
- - -


10. 과적합 방지 모델의 과적합을 방지하기 위한 여러 방법들이 있습니다.
정규화(Regularization): L1, L2 정규화를 통해 모델의 복잡도를 조절하여 과적합을 방지합니다.
드롭아웃(Dropout): 신경망에서 드롭아웃을 사용하여 과적합을 방지할 수 있습니다.
얼리 스탑핑(Early Stopping): 검증 데이터의 성능이 더 이상 개선되지 않으면 훈련을 조기에 중지합니다.
> 🧞‍♀️ 과적합 방지를 위해 도입한 방법이 있나요? 그 방법의 원리와 장점은 무엇인가요?

